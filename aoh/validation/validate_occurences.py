import argparse
import json
import os
from functools import partial
from multiprocessing import cpu_count, Pool
from pathlib import Path

import geopandas as gpd
import pandas as pd
import yirgacheffe as yg
from shapely.geometry import Point

def process_species(
    aohs_path: Path,
    species_data_path: Path,
    species_occurences: pd.DataFrame,
) -> tuple[int, int, int, float, float, bool] | None:

    os.environ["OGR_GEOJSON_MAX_OBJ_SIZE"] = "0"

    if len(species_occurences) == 0:
        return None

    taxon_ids = species_occurences.iucn_taxon_id.unique()
    if len(taxon_ids) > 1:
        raise ValueError("Too many taxon IDs")
    taxon_id = taxon_ids[0]

    aoh_files = list(aohs_path.glob(f"**/{taxon_id}*.tif"))
    # We here are aborting on those species with no data or those
    # with multiple seasons
    if len(aoh_files) != 1:
        return None

    aoh_tiff_path = aoh_files[0]
    aoh_data_path = aoh_tiff_path.with_suffix(".json")
    with open(aoh_data_path, 'r', encoding='utf-8') as f:
        aoh_data = json.load(f)

    species_data_files = list(species_data_path.glob(f"**/{taxon_id}*.geojson"))
    if len(species_data_files) != 1:
        return None
    species_range = gpd.read_file(species_data_files[0])

    # From Dahal et al: "This ensured that we only included points which fell inside
    #Â the boundaries of the selected range maps."
    points_gdf = gpd.GeoDataFrame(
        species_occurences,
        geometry=[
            Point(lon, lat)
            for lon, lat in
            zip(species_occurences['decimalLongitude'], species_occurences['decimalLatitude'])
        ],
        crs='EPSG:4326',
    )
    clipped_points = gpd.sjoin(points_gdf, species_range, predicate='within', how='inner')

    pixel_set = set()
    with yg.read_raster(aoh_files[0]) as aoh:
        results = []
        for _, row in clipped_points.iterrows():
            pixel_x, pixel_y = aoh.pixel_for_latlng(row.decimalLatitude, row.decimalLongitude)

            # Dahal et al: "We also made sure that only one point locality was allowed per pixel of
            # the AOH map to avoid clustering of points."
            if (pixel_x, pixel_y) in pixel_set:
                continue
            pixel_set.add((pixel_x, pixel_y))

            value = aoh.read_array(pixel_x, pixel_y, 1, 1)
            results.append(value[0][0] > 0.0)

    # From Dahal et al: "Finally, we excluded species which had fewer than 10 point localities after
    # all the filters were applied."
    if len(results) < 10:
        return None

    matches = len([x for x in results if x])
    point_prevalence = matches / len(results)
    model_prevalence = aoh_data['prevalence']
    return (
        taxon_id,
        len(results),
        matches,
        point_prevalence,
        model_prevalence,
        point_prevalence <= model_prevalence
    )

def validate_occurences(
    gbif_data_path: Path,
    aohs_path: Path,
    species_data_path: Path,
    output_path: Path,
    process_count: int,
) -> None:
    os.makedirs(output_path.parent, exist_ok=True)

    # The input is from the points.csv generated by fetch_gbif_data.py, which has the columns:
    # iucn_taxon_id, gbif_id, decimalLatitude, decimalLongitude, assessment year
    occurences = pd.read_csv(gbif_data_path)
    occurences.drop(columns=['gbif_id', 'year'], inplace=True)
    occurences.sort_values(['iucn_taxon_id', 'decimalLatitude'], inplace=True)
    occurences_per_species = [group for _, group in occurences.groupby('iucn_taxon_id')]
    with Pool(processes=process_count) as pool:
        results_per_species = pool.map(partial(process_species, aohs_path, species_data_path), occurences_per_species)
    cleaned_results = [x for x in results_per_species if x is not None]

    summary = pd.DataFrame(cleaned_results, columns=[
        "id_no",
        "occurrences",
        "in_aoh",
        "point prevalence",
        "model prevalence",
        "outlier",
    ])
    summary = summary[summary.outlier == True]
    summary.to_csv(output_path, index=False)

def main() -> None:
    parser = argparse.ArgumentParser(description="Validate map prevalence.")
    parser.add_argument(
        '--gbif_data_path',
        type=Path,
        help="Data containing downloaded GBIF data.",
        required=True,
        dest="gbif_data_path"
    )
    parser.add_argument(
        '--species_data',
        type=Path,
        help="Path of all the species range data.",
        required=True,
        dest="species_data_path",
    )
    parser.add_argument(
        '--aoh_results',
        type=Path,
        help="Path of all the AoH outputs.",
        required=True,
        dest="aohs_path"
    )
    parser.add_argument(
        "--output",
        type=Path,
        required=True,
        dest="output_path",
        help="CSV of outliers."
    )
    parser.add_argument(
        "-j",
        type=int,
        required=False,
        default=round(cpu_count() / 2),
        dest="processes_count",
        help="Optional number of concurrent threads to use."
    )
    args = parser.parse_args()

    validate_occurences(
        args.gbif_data_path,
        args.aohs_path,
        args.species_data_path,
        args.output_path,
        args.processes_count,
    )

if __name__ == "__main__":
    main()
